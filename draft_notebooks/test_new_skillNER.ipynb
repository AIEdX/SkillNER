{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# change working directory to root directory\r\n",
    "import os\r\n",
    "\r\n",
    "os.chdir(\"../\")\r\n",
    "os.getcwd()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/mac/Desktop/msda_labour_graph/MSDA jobs graph '"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "This notebook will be a sandbox to test the integration of new skillNer modules and source for the package migration "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## only run when semsi update their data hhhhhhh ! (it takes a lot of time trust me )\r\n",
    "\"\"\"\r\n",
    "from skillNer.fetch import fetch_skills_list\r\n",
    "# load skill list \r\n",
    "response = fetch_skills_list()\r\n",
    "import re\r\n",
    "skills_dict = {}\r\n",
    "for i,skill in enumerate(response) : \r\n",
    "    print(i/len(response),skill)\r\n",
    "    print('-------------')\r\n",
    "    # get info \r\n",
    "    skill_id = skill['id']\r\n",
    "    skill_name = skill['name']\r\n",
    "    skill_type = skill['type']['name']\r\n",
    "    # clean definitions and textify \r\n",
    "    cleaned_skill_name = re.sub(r\"\\([^()]*\\)\", \"\", skill_name)\r\n",
    "    skill_obj = Text(cleaned_skill_name)\r\n",
    "    # add to dict \r\n",
    "    skills_dict[skill_id] = {'skill_id':skill_id,\r\n",
    "                             'skill_name' :skill_name , \r\n",
    "                             'skill_type':skill_type,\r\n",
    "                             'skill_obj':skill_obj }\r\n",
    "# store ready to use skill dict\r\n",
    "final_skill_dict = {}\r\n",
    "for skill_key in skills_dict.keys() :\r\n",
    "            skill_name = skills_dict[skill_key]['skill_name']\r\n",
    "            skill_obj = skills_dict[skill_key]['skill_obj']\r\n",
    "            final_skill_dict[skill_key] = {'skill_name' :skill_name , \r\n",
    "                                     'skill_cleaned':skill_obj.transformed_text,\r\n",
    "                                     'skill_type':skill_type,\r\n",
    "                                     'skill_lemmed':skill_obj.lemmed(),\r\n",
    "                                     'skill_stemmed':skill_obj.stemmed(),\r\n",
    "                                     'skill_len' : len(skill_obj)\r\n",
    "                                   }\r\n",
    "            \r\n",
    "import json\r\n",
    "with open('./skillNer/skills_processed.json', 'w', encoding='utf-8') as f:\r\n",
    "    json.dump(final_skill_dict, f, ensure_ascii=False, indent=4)\r\n",
    "\"\"\"\r\n",
    "print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# load libs \r\n",
    "import pprint\r\n",
    "from skillNer.text_class import Text\r\n",
    "\r\n",
    "import spacy\r\n",
    "from spacy.matcher import PhraseMatcher\r\n",
    "nlp = spacy.load(\"en_core_web_lg\")\r\n",
    "# load stop_words to be filtred for n_gram (should be decripted in future)\r\n",
    "from nltk.corpus import stopwords\r\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# import processed skills db \r\n",
    "import json\r\n",
    "\r\n",
    "with open('./skillNer/skills_processed.json') as json_file:\r\n",
    "    skills_db = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "skills_db['ES7947C5DF69201A2341']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'skill_name': 'Airworthiness Engineering',\n",
       " 'skill_cleaned': 'airworthiness engineering',\n",
       " 'skill_type': 'Hard Skill',\n",
       " 'skill_lemmed': 'airworthiness engineering',\n",
       " 'skill_stemmed': 'airworthi engin',\n",
       " 'skill_len': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## dist used in n_gram_process should be stored as skills_db since its fixed !\r\n",
    "sgrams = [skills_db[key]['skill_stemmed'] for key in skills_db if skills_db[key]['skill_len']==2]\r\n",
    "def get_dist(array):\r\n",
    "    words = []\r\n",
    "    for val in array :\r\n",
    "        vals = val.split(' ')\r\n",
    "        for v in vals :\r\n",
    "            \r\n",
    "            words.append(v)\r\n",
    "    \r\n",
    "    import collections\r\n",
    "    \r\n",
    "    a = words\r\n",
    "    counter=collections.Counter(a)\r\n",
    "    counter = dict(counter)\r\n",
    "   # print(counter)\r\n",
    "    max_ = max(list(counter.values()))\r\n",
    "    min_ = min(list(counter.values()))\r\n",
    "\r\n",
    "    if max_==min_ :\r\n",
    "        ret = counter\r\n",
    "    else :\r\n",
    "        ret  = {k: 1-((v-min_)/(max_-min_)) for k, v in counter.items()}\r\n",
    "    return ret\r\n",
    "sgrams_skills_tokens_dist =  get_dist(sgrams)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "# load matchers \r\n",
    "def load_matchers(include=['full_matcher','ngram_matcher','uni_gram_matcher']):\r\n",
    "    # creators \r\n",
    "    def get_full_matcher():\r\n",
    "        # init matcher \r\n",
    "        full_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\r\n",
    "        # populate matcher\r\n",
    "        for key in skills_db :\r\n",
    "            # get skill info \r\n",
    "            skill_id =key\r\n",
    "            skill_full_name = skills_db[key]['skill_cleaned']\r\n",
    "            skill_len = skills_db[key]['skill_len']\r\n",
    "            if skill_len>1 :\r\n",
    "                # add to matcher\r\n",
    "                skill_full_name_spacy = nlp.make_doc(skill_full_name)\r\n",
    "                full_matcher.add(str(skill_id), [skill_full_name_spacy])\r\n",
    "                \r\n",
    "        return full_matcher\r\n",
    "    def get_ngram_matcher():\r\n",
    "        ## init matcher\r\n",
    "        ngram_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\r\n",
    "        # populate matcher\r\n",
    "        for key in skills_db :\r\n",
    "            # get skill info \r\n",
    "            skill_id =key\r\n",
    "            skill_lemmed = skills_db[key]['skill_lemmed']\r\n",
    "            skill_lemmed_tokens = [w for w in skill_lemmed.split(' ') \r\n",
    "                                    if not(w in stop_words or w.isdigit())]\r\n",
    "            \r\n",
    "            skill_len = skills_db[key]['skill_len']\r\n",
    "            if skill_len >1 :## add only ngram skills \r\n",
    "                # add full_stemed to matcher\r\n",
    "                skill_lemmed_spacy = nlp.make_doc(skill_lemmed)\r\n",
    "                ngram_matcher.add(str(skill_id), [skill_lemmed_spacy])\r\n",
    "                ## add tokens to matcher \r\n",
    "                for token in skill_lemmed_tokens :\r\n",
    "                    ## give id that ref 1_gram matching\r\n",
    "                    id_ = skill_id+'_1w'\r\n",
    "                    ngram_matcher.add(str(id_), [nlp.make_doc(token)])\r\n",
    "                \r\n",
    "        return ngram_matcher\r\n",
    "    def get_uni_gram__matcher():\r\n",
    "        # init matcher \r\n",
    "        single_gram_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\r\n",
    "        # populate matcher\r\n",
    "        for key in skills_db :\r\n",
    "            # get skill info \r\n",
    "            skill_id =key\r\n",
    "            skill_stemmed = skills_db[key]['skill_stemmed']\r\n",
    "            skill_len = skills_db[key]['skill_len']\r\n",
    "            if skill_len==1 :\r\n",
    "                # add to matcher\r\n",
    "                skill_stemmed_spacy = nlp.make_doc(skill_stemmed)\r\n",
    "                single_gram_matcher.add(str(skill_id), [skill_stemmed_spacy])\r\n",
    "                \r\n",
    "        return single_gram_matcher    \r\n",
    "    \r\n",
    "    load_matcher = {\r\n",
    "        'full_matcher' :get_full_matcher,\r\n",
    "        'ngram_matcher' : get_ngram_matcher,\r\n",
    "        'uni_gram_matcher' : get_uni_gram__matcher,\r\n",
    "    }\r\n",
    "    \r\n",
    "    # load to return \r\n",
    "    loader = {}\r\n",
    "    for type_ in include :\r\n",
    "        print('loading ',type_,\"...\")\r\n",
    "        loader[type_] = load_matcher[type_]()\r\n",
    "    return loader\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_full_match_skills(text_obj,matcher):\r\n",
    "    skills = []\r\n",
    "    doc = nlp(text_obj.transformed_text)\r\n",
    "    \r\n",
    "    for match_id, start, end in matcher(doc):\r\n",
    "        id_ = matcher.vocab.strings[match_id]\r\n",
    "        ## add full_match to store\r\n",
    "        skills.append({'skill_id':id_,\r\n",
    "                       'doc_node_value':str(doc[start:end]),\r\n",
    "                       'doc_node_id':list(range(start,end))})\r\n",
    "        ## mutate text tokens metadata (unmatch attr) \r\n",
    "        for token in text_obj[start:end] :\r\n",
    "            token.is_matchable = False\r\n",
    "        \r\n",
    "    return skills,text_obj\r\n",
    "\r\n",
    "def get_sub_match_skills(text_obj,matcher):\r\n",
    "    skills_full = []\r\n",
    "    skills = []\r\n",
    "    sub_matches= []\r\n",
    "    full_matches = []\r\n",
    "    doc = nlp(text_obj.lemmed())\r\n",
    "    for match_id, start, end in matcher(doc):\r\n",
    "        id_ = matcher.vocab.strings[match_id]\r\n",
    "        if '_1w' in id_ : \r\n",
    "            sub_matches.append((id_,match_id, start, end))\r\n",
    "        else : \r\n",
    "            full_matches.append((id_,match_id, start, end))\r\n",
    "            \r\n",
    "            \r\n",
    "    for match in  full_matches : \r\n",
    "            id_,match_id, start, end = match\r\n",
    "            ## full matches no need for scoring\r\n",
    "            ## check if any intersection betwenn full matcher and sub matcher (priority to full)\r\n",
    "            is_matchable = [1 for token in  text_obj[start:end] if token.is_matchable]\r\n",
    "            if len(is_matchable)!= 0 :\r\n",
    "                \r\n",
    "                 skills_full.append({'skill_id':id_,\r\n",
    "                                      'doc_node_value':str(doc[start:end]),\r\n",
    "                                      'doc_node_id':list(range(start,end))})\r\n",
    "     \r\n",
    "                 ## mutate text tokens metadata (unmatch attr) - only in full match stemmed (100% confident )\r\n",
    "                 for token in text_obj[start:end] :\r\n",
    "                      token.is_matchable = False  \r\n",
    "                    \r\n",
    "            \r\n",
    "    for match in sub_matches:\r\n",
    "            id_,match_id, start, end = match\r\n",
    "            # add unigram macthes only if not matched in parent modules or not stop word\r\n",
    "            if text_obj[start].is_matchable and (not text_obj[start].is_stop_word):\r\n",
    "                skills.append({'skill_id':id_,\r\n",
    "                               'doc_node_value':str(doc[start:end]),\r\n",
    "                               'doc_node_id':start})\r\n",
    "                \r\n",
    "    return skills_full,skills,text_obj\r\n",
    "\r\n",
    "def get_single_match_skills(text_obj,matcher):\r\n",
    "    skills = []\r\n",
    "   \r\n",
    "    doc = nlp(text_obj.stemmed())\r\n",
    "    for match_id, start, end in matcher(doc):\r\n",
    "        id_ = matcher.vocab.strings[match_id]\r\n",
    "        if text_obj[start].is_matchable : \r\n",
    "            skills.append({'skill_id':id_,\r\n",
    "                           'doc_node_value':str(doc[start:end]),\r\n",
    "                           'doc_node_id':start})\r\n",
    "\r\n",
    "    return skills"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# process_n_gram ,process_uni_gram : filter and score  \r\n",
    "## utils \r\n",
    "import numpy as np\r\n",
    "def one_gram_sim(text_str,skill_str):\r\n",
    "    # transform into sentence \r\n",
    "    text = text_str+' '+skill_str\r\n",
    "    tokens = nlp(text)\r\n",
    "    token1, token2 = tokens[0], tokens[1]\r\n",
    "   # print(token1, token2)\r\n",
    "    return token1.similarity(token2)\r\n",
    "def similarity(texts):\r\n",
    "    doc1 = nlp(texts[0])\r\n",
    "    doc2 = nlp(texts[1])\r\n",
    " \r\n",
    "\r\n",
    "    return  doc1.similarity(doc2)\r\n",
    "def get_sim(skill_name, f ,input_text):\r\n",
    "    text = skill_name.lower().replace(f,'').strip()\r\n",
    "    #print('props : ', skill_name,'/', f ,'/',input_text)\r\n",
    "    #print('sim /',text,'/',input_text)\r\n",
    "    return similarity([text,input_text])\r\n",
    "\r\n",
    "def make_one(cluster,len_):\r\n",
    "    a = [1]*len_\r\n",
    "    return [1*(i in cluster) for i,one in enumerate(a) ]\r\n",
    "def split_at_values(lst, val):\r\n",
    "    return [i for i, x in enumerate(lst) if x != val]\r\n",
    "        \r\n",
    "def grouper(iterable,dist):\r\n",
    "    prev = None\r\n",
    "    group = []\r\n",
    "    for item in iterable:\r\n",
    "        if  prev==None or item - prev <= dist:\r\n",
    "            group.append(item)\r\n",
    "        else:\r\n",
    "            yield group\r\n",
    "            group = [item]\r\n",
    "        prev = item\r\n",
    "    if group:\r\n",
    "        yield group \r\n",
    "def get_clusters (co_oc):\r\n",
    "    clusters = []\r\n",
    "    for i,row in enumerate(co_oc):\r\n",
    "        clusts = list(grouper(split_at_values(row, 0),1))\r\n",
    "        if clusts != [] :\r\n",
    "            #print(i,[c for c in clusts if i in c][0])\r\n",
    "            a = [c for c in clusts if i in c][0]\r\n",
    "            if a not in clusters :\r\n",
    "                \r\n",
    "                clusters.append(a)\r\n",
    "    return clusters\r\n",
    "def is_low_frequency(match_str,skill_id):\r\n",
    "    skill_name = skills_db[skill_id]['skill_stemmed'].split(' ')\r\n",
    "  \r\n",
    "    if sgrams_skills_tokens_dist[skill_name[0]]>= sgrams_skills_tokens_dist[skill_name[1]]:\r\n",
    "      \r\n",
    "        return skill_name[0]==match_str\r\n",
    "    else : \r\n",
    "        return skill_name[1]==match_str\r\n",
    "def retain(text , tokens,skill_id,sk_look,corpus ) :\r\n",
    "    ## get id \r\n",
    "    real_id = sk_look[skill_id].split('_1w')[0]\r\n",
    "    ## get len \r\n",
    "    len_ = skills_db[real_id]['skill_len']\r\n",
    "    len_condition = corpus[skill_id].dot(tokens)/len_\r\n",
    "    \r\n",
    "    s_gr = np.array(list(tokens))*np.array(list(corpus[skill_id]))\r\n",
    "    def condition(x): return x == 1\r\n",
    "    s_gr_ind = [idx for idx, element in enumerate(s_gr) if condition(element)][0]\r\n",
    "    #print('len',len_,real_id,s_gr_ind )\r\n",
    "    if len_condition >=0.5 : \r\n",
    "        #print('debug',debug[real_id] ,corpus[skill_id] ,tokens , len_condition  )\r\n",
    "        if len_>2   :\r\n",
    "                #print('ngram', True , real_id)\r\n",
    "                score = len_condition\r\n",
    "                return (True , {'skill_id':real_id,\r\n",
    "                                'doc_node_id':  [i for i,val in enumerate(s_gr) if val==1],\r\n",
    "                                'score':round(score,2)\r\n",
    "                               })\r\n",
    "            \r\n",
    "        if  is_low_frequency(text[s_gr_ind],real_id)   :\r\n",
    "                #print('2gram', look_up_ngram[real_id],\"/\", text[s_gr_ind] ,\"/\",' '.join(text))\r\n",
    "                score = get_sim(skills_db[real_id]['skill_lemmed'], text[s_gr_ind] ,' '.join(text))\r\n",
    "                return (True , {'skill_id':real_id,\r\n",
    "                                'score':round(score,2),\r\n",
    "                                 'doc_node_id': [i for i,val in enumerate(s_gr) if val==1]\r\n",
    "                               })\r\n",
    "        return (False , '')\r\n",
    "\r\n",
    "    else :\r\n",
    "        return (False , '')\r\n",
    "    \r\n",
    "def get_corpus(text,matches):\r\n",
    "    len_ = len(text)\r\n",
    "    corpus = []\r\n",
    "    look_up = {}\r\n",
    "    unique_skills = list(set([match['skill_id'] for match in matches]))\r\n",
    "    skill_text_match_bin = [0]*len_\r\n",
    "    for index,skill_id in enumerate(unique_skills) : \r\n",
    "        \r\n",
    "        on_inds = [match['doc_node_id'] for match in matches if match['skill_id']==skill_id]\r\n",
    "        skill_text_match_bin_updated = [(i in on_inds)*1 for i,_ in enumerate(skill_text_match_bin)]\r\n",
    "        corpus.append(skill_text_match_bin_updated)\r\n",
    "        look_up[index] = skill_id\r\n",
    "                   \r\n",
    "    return np.array(corpus) , look_up\r\n",
    "    \r\n",
    "## main functions\r\n",
    "def process_n_gram(matches,text_obj):\r\n",
    "                if len(matches)==0:\r\n",
    "                     return matches\r\n",
    "                \r\n",
    "                # get text spans with  conflict \r\n",
    "                text_tokens = text_obj.lemmed().split(' ')\r\n",
    "                len_ = len(text_tokens)\r\n",
    "                ## create corpus matrix  \r\n",
    "                corpus,look_up = get_corpus(text_tokens,matches)\r\n",
    "                ## generate spans \r\n",
    "                co_occ = np.matmul(corpus.T, corpus) # co-occurence of tokens aij : co-occurence of token i with token j\r\n",
    "                clusters = get_clusters(co_occ)\r\n",
    "                ones = [ make_one(cluster,len_) for cluster in clusters]\r\n",
    "                spans = [(np.array(one),np.array([a_[0] for a_ in np.argwhere(corpus.dot(one)!=0)])) \r\n",
    "                         for one in ones]\r\n",
    "                # filter and score \r\n",
    "                new_spans = []\r\n",
    "                for span in spans :\r\n",
    "                    tokens,skill_ids = span \r\n",
    "                    new_skill_obj = []\r\n",
    "                    for sk_id in skill_ids : \r\n",
    "                        retain_ , r_sk_id = retain(text_tokens , tokens,sk_id,look_up,corpus )\r\n",
    "                        if   retain_ :\r\n",
    "                            new_skill_obj.append(r_sk_id)\r\n",
    "                    # get max for each span \r\n",
    "                    scores = [sk['score'] for sk in new_skill_obj]\r\n",
    "                    if scores!=[]:\r\n",
    "                        max_score_index = np.array(scores).argmax() \r\n",
    "                \r\n",
    "                        new_spans.append(new_skill_obj[max_score_index])                \r\n",
    "                \r\n",
    "                return new_spans\r\n",
    "            \r\n",
    "def process_unigram(matches,text_obj):\r\n",
    "                original_text = text_obj.transformed_text.split(' ')                \r\n",
    "                res = []\r\n",
    "                for match in matches :\r\n",
    "                    id_ = match['skill_id']\r\n",
    "                    match_id = match['doc_node_id']\r\n",
    "\r\n",
    "                    skill_str = skills_db[id_]['skill_cleaned']\r\n",
    "                    #print(match_id)\r\n",
    "                    text_str = original_text[match_id]\r\n",
    "                    sim = one_gram_sim(skill_str,text_str)\r\n",
    "                    #match['doc_node_id'] = [match['doc_node_id']] # for normalisation purpose\r\n",
    "\r\n",
    "                        \r\n",
    "                    res.append({'skill_id':id_,\r\n",
    "                                'doc_node_id':[match_id],\r\n",
    "                                'doc_node_value':match['doc_node_value'],\r\n",
    "                                'score':round(sim,2)})  \r\n",
    "                return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Skills extractor test  :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "matchers = load_matchers()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading  full_matcher ...\n",
      "loading  ngram_matcher ...\n",
      "loading  uni_gram_matcher ...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "## get matches \r\n",
    "input_ = 'Hey, Geeks !, How are you?, you having a good mastery of English language as well experience in'\r\n",
    "text = Text(input_)\r\n",
    "\r\n",
    "skills_full,text = get_full_match_skills(text,matchers['full_matcher'])\r\n",
    "skills_sub_full,skills_ngram,text = get_sub_match_skills(text,matchers['ngram_matcher'])\r\n",
    "skills_uni = get_single_match_skills(text,matchers['uni_gram_matcher'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "## process \r\n",
    "n_gram_pro = process_n_gram(skills_ngram,text)\r\n",
    "uni_gram_pro = process_unigram(skills_uni,text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "## results \r\n",
    "pprint.pprint({'full_match':skills_full,\r\n",
    "               'ngram_full_match':skills_sub_full,\r\n",
    "               'ngram_scored':n_gram_pro,\r\n",
    "               'unigram_scored':uni_gram_pro\r\n",
    "              })"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'full_match': [{'doc_node_id': [11, 12],\n",
      "                 'doc_node_value': 'english language',\n",
      "                 'skill_id': 'KS123K75YYK8VGH90NCS'}],\n",
      " 'ngram_full_match': [],\n",
      " 'ngram_scored': [],\n",
      " 'unigram_scored': []}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
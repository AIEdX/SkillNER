{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mac/Desktop/msda_labour_graph/MSDA jobs graph '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change working directory to root directory\n",
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will be a sandbox to test the integration of new skillNer modules and source for the package migration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## only run when semsi update their data hhhhhhh ! (it takes a lot of time trust me )\n",
    "\"\"\"\n",
    "from skillNer.fetch import fetch_skills_list\n",
    "# load skill list \n",
    "response = fetch_skills_list()\n",
    "import re\n",
    "skills_dict = {}\n",
    "for i,skill in enumerate(response) : \n",
    "    print(i/len(response),skill)\n",
    "    print('-------------')\n",
    "    # get info \n",
    "    skill_id = skill['id']\n",
    "    skill_name = skill['name']\n",
    "    skill_type = skill['type']['name']\n",
    "    # clean definitions and textify \n",
    "    cleaned_skill_name = re.sub(r\"\\([^()]*\\)\", \"\", skill_name)\n",
    "    skill_obj = Text(cleaned_skill_name)\n",
    "    # add to dict \n",
    "    skills_dict[skill_id] = {'skill_id':skill_id,\n",
    "                             'skill_name' :skill_name , \n",
    "                             'skill_type':skill_type,\n",
    "                             'skill_obj':skill_obj }\n",
    "# store ready to use skill dict\n",
    "final_skill_dict = {}\n",
    "for skill_key in skills_dict.keys() :\n",
    "            skill_name = skills_dict[skill_key]['skill_name']\n",
    "            skill_obj = skills_dict[skill_key]['skill_obj']\n",
    "            final_skill_dict[skill_key] = {'skill_name' :skill_name , \n",
    "                                     'skill_cleaned':skill_obj.transformed_text,\n",
    "                                     'skill_type':skill_type,\n",
    "                                     'skill_lemmed':skill_obj.lemmed(),\n",
    "                                     'skill_stemmed':skill_obj.stemmed(),\n",
    "                                     'skill_len' : len(skill_obj)\n",
    "                                   }\n",
    "            \n",
    "import json\n",
    "with open('./skillNer/skills_processed.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_skill_dict, f, ensure_ascii=False, indent=4)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs \n",
    "import pprint\n",
    "from skillNer.text_class import Text,nlp\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "# load stop_words to be filtred for n_gram (should be decripted in future)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs \n",
    "import pprint\n",
    "from skillNer.text_class import Text\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# load stop_words to be filtred for n_gram (should be decripted in future)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import processed skills db \n",
    "import json\n",
    "\n",
    "with open('./skillNer/skills_processed.json') as json_file:\n",
    "    skills_db = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skill_name': 'Airworthiness Engineering',\n",
       " 'skill_cleaned': 'airworthiness engineering',\n",
       " 'skill_type': 'Hard Skill',\n",
       " 'skill_lemmed': 'airworthiness engineering',\n",
       " 'skill_stemmed': 'airworthi engin',\n",
       " 'skill_len': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_db['ES7947C5DF69201A2341']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dist used in n_gram_process should be stored as skills_db since its fixed !\n",
    "sgrams = [skills_db[key]['skill_stemmed'] for key in skills_db if skills_db[key]['skill_len']==2]\n",
    "def get_dist(array):\n",
    "    words = []\n",
    "    for val in array :\n",
    "        vals = val.split(' ')\n",
    "        for v in vals :\n",
    "            \n",
    "            words.append(v)\n",
    "    \n",
    "    import collections\n",
    "    \n",
    "    a = words\n",
    "    counter=collections.Counter(a)\n",
    "    counter = dict(counter)\n",
    "   # print(counter)\n",
    "    max_ = max(list(counter.values()))\n",
    "    min_ = min(list(counter.values()))\n",
    "\n",
    "    if max_==min_ :\n",
    "        ret = counter\n",
    "    else :\n",
    "        ret  = {k: 1-((v-min_)/(max_-min_)) for k, v in counter.items()}\n",
    "    return ret\n",
    "sgrams_skills_tokens_dist =  get_dist(sgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load matchers \n",
    "def load_matchers(include=['full_matcher','ngram_matcher','uni_gram_matcher']):\n",
    "    # creators \n",
    "    def get_full_matcher():\n",
    "        # init matcher \n",
    "        full_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        # populate matcher\n",
    "        for key in skills_db :\n",
    "            # get skill info \n",
    "            skill_id =key\n",
    "            skill_full_name = skills_db[key]['skill_cleaned']\n",
    "            skill_len = skills_db[key]['skill_len']\n",
    "            if skill_len>1 :\n",
    "                # add to matcher\n",
    "                skill_full_name_spacy = nlp.make_doc(skill_full_name)\n",
    "                full_matcher.add(str(skill_id), [skill_full_name_spacy])\n",
    "                \n",
    "        return full_matcher\n",
    "    def get_ngram_matcher():\n",
    "        ## init matcher\n",
    "        ngram_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        # populate matcher\n",
    "        for key in skills_db :\n",
    "            # get skill info \n",
    "            skill_id =key\n",
    "            skill_lemmed = skills_db[key]['skill_lemmed']\n",
    "            skill_lemmed_tokens = [w for w in skill_lemmed.split(' ') \n",
    "                                    if not(w in stop_words or w.isdigit())]\n",
    "            \n",
    "            skill_len = skills_db[key]['skill_len']\n",
    "            if skill_len >1 :## add only ngram skills \n",
    "                # add full_stemed to matcher\n",
    "                skill_lemmed_spacy = nlp.make_doc(skill_lemmed)\n",
    "                ngram_matcher.add(str(skill_id), [skill_lemmed_spacy])\n",
    "                ## add tokens to matcher \n",
    "                for token in skill_lemmed_tokens :\n",
    "                    ## give id that ref 1_gram matching\n",
    "                    id_ = skill_id+'_1w'\n",
    "                    ngram_matcher.add(str(id_), [nlp.make_doc(token)])\n",
    "                \n",
    "        return ngram_matcher\n",
    "    def get_uni_gram__matcher():\n",
    "        # init matcher \n",
    "        single_gram_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        # populate matcher\n",
    "        for key in skills_db :\n",
    "            # get skill info \n",
    "            skill_id =key\n",
    "            skill_stemmed = skills_db[key]['skill_stemmed']\n",
    "            skill_len = skills_db[key]['skill_len']\n",
    "            if skill_len==1 :\n",
    "                # add to matcher\n",
    "                skill_stemmed_spacy = nlp.make_doc(skill_stemmed)\n",
    "                single_gram_matcher.add(str(skill_id), [skill_stemmed_spacy])\n",
    "                \n",
    "        return single_gram_matcher    \n",
    "    \n",
    "    load_matcher = {\n",
    "        'full_matcher' :get_full_matcher,\n",
    "        'ngram_matcher' : get_ngram_matcher,\n",
    "        'uni_gram_matcher' : get_uni_gram__matcher,\n",
    "    }\n",
    "    \n",
    "    # load to return \n",
    "    loader = {}\n",
    "    for type_ in include :\n",
    "        print('loading ',type_,\"...\")\n",
    "        loader[type_] = load_matcher[type_]()\n",
    "    return loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_match_skills(text_obj,matcher):\n",
    "    skills = []\n",
    "    doc = nlp(text_obj.transformed_text)\n",
    "    \n",
    "    for match_id, start, end in matcher(doc):\n",
    "        id_ = matcher.vocab.strings[match_id]\n",
    "        ## add full_match to store\n",
    "        skills.append({'skill_id':id_,\n",
    "                       'doc_node_value':str(doc[start:end]),\n",
    "                       'doc_node_id':list(range(start,end))})\n",
    "        ## mutate text tokens metadata (unmatch attr) \n",
    "        for token in text_obj[start:end] :\n",
    "            token.is_matchable = False\n",
    "        \n",
    "    return skills,text_obj\n",
    "\n",
    "def get_sub_match_skills(text_obj,matcher):\n",
    "    skills_full = []\n",
    "    skills = []\n",
    "    sub_matches= []\n",
    "    full_matches = []\n",
    "    doc = nlp(text_obj.lemmed())\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        id_ = matcher.vocab.strings[match_id]\n",
    "        if '_1w' in id_ : \n",
    "            sub_matches.append((id_,match_id, start, end))\n",
    "        else : \n",
    "            full_matches.append((id_,match_id, start, end))\n",
    "            \n",
    "            \n",
    "    for match in  full_matches : \n",
    "            id_,match_id, start, end = match\n",
    "            ## full matches no need for scoring\n",
    "            ## check if any intersection betwenn full matcher and sub matcher (priority to full)\n",
    "            is_matchable = [1 for token in  text_obj[start:end] if token.is_matchable]\n",
    "            if len(is_matchable)!= 0 :\n",
    "                \n",
    "                 skills_full.append({'skill_id':id_,\n",
    "                                      'doc_node_value':str(doc[start:end]),\n",
    "                                      'doc_node_id':list(range(start,end))})\n",
    "     \n",
    "                 ## mutate text tokens metadata (unmatch attr) - only in full match stemmed (100% confident )\n",
    "                 for token in text_obj[start:end] :\n",
    "                      token.is_matchable = False  \n",
    "                    \n",
    "            \n",
    "    for match in sub_matches:\n",
    "            id_,match_id, start, end = match\n",
    "            # add unigram macthes only if not matched in parent modules or not stop word\n",
    "            if text_obj[start].is_matchable and (not text_obj[start].is_stop_word):\n",
    "                skills.append({'skill_id':id_,\n",
    "                               'doc_node_value':str(doc[start:end]),\n",
    "                               'doc_node_id':start})\n",
    "                \n",
    "    return skills_full,skills,text_obj\n",
    "\n",
    "def get_single_match_skills(text_obj,matcher):\n",
    "    skills = []\n",
    "   \n",
    "    doc = nlp(text_obj.stemmed())\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        id_ = matcher.vocab.strings[match_id]\n",
    "        if text_obj[start].is_matchable : \n",
    "            skills.append({'skill_id':id_,\n",
    "                           'doc_node_value':str(doc[start:end]),\n",
    "                           'doc_node_id':start})\n",
    "\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_n_gram ,process_uni_gram : filter and score  \n",
    "## utils \n",
    "import numpy as np\n",
    "def one_gram_sim(text_str,skill_str):\n",
    "    # transform into sentence \n",
    "    text = text_str+' '+skill_str\n",
    "    tokens = nlp(text)\n",
    "    token1, token2 = tokens[0], tokens[1]\n",
    "   # print(token1, token2)\n",
    "    return token1.similarity(token2)\n",
    "def similarity(texts):\n",
    "    doc1 = nlp(texts[0])\n",
    "    doc2 = nlp(texts[1])\n",
    " \n",
    "\n",
    "    return  doc1.similarity(doc2)\n",
    "def get_sim(skill_name, f ,input_text):\n",
    "    text = skill_name.lower().replace(f,'').strip()\n",
    "    #print('props : ', skill_name,'/', f ,'/',input_text)\n",
    "    #print('sim /',text,'/',input_text)\n",
    "    return similarity([text,input_text])\n",
    "\n",
    "def make_one(cluster,len_):\n",
    "    a = [1]*len_\n",
    "    return [1*(i in cluster) for i,one in enumerate(a) ]\n",
    "def split_at_values(lst, val):\n",
    "    return [i for i, x in enumerate(lst) if x != val]\n",
    "        \n",
    "def grouper(iterable,dist):\n",
    "    prev = None\n",
    "    group = []\n",
    "    for item in iterable:\n",
    "        if  prev==None or item - prev <= dist:\n",
    "            group.append(item)\n",
    "        else:\n",
    "            yield group\n",
    "            group = [item]\n",
    "        prev = item\n",
    "    if group:\n",
    "        yield group \n",
    "def get_clusters (co_oc):\n",
    "    clusters = []\n",
    "    for i,row in enumerate(co_oc):\n",
    "        clusts = list(grouper(split_at_values(row, 0),1))\n",
    "        if clusts != [] :\n",
    "            #print(i,[c for c in clusts if i in c][0])\n",
    "            a = [c for c in clusts if i in c][0]\n",
    "            if a not in clusters :\n",
    "                \n",
    "                clusters.append(a)\n",
    "    return clusters\n",
    "def is_low_frequency(match_str,skill_id):\n",
    "    skill_name = skills_db[skill_id]['skill_stemmed'].split(' ')\n",
    "  \n",
    "    if sgrams_skills_tokens_dist[skill_name[0]]>= sgrams_skills_tokens_dist[skill_name[1]]:\n",
    "      \n",
    "        return skill_name[0]==match_str\n",
    "    else : \n",
    "        return skill_name[1]==match_str\n",
    "def retain(text , tokens,skill_id,sk_look,corpus ) :\n",
    "    ## get id \n",
    "    real_id = sk_look[skill_id].split('_1w')[0]\n",
    "    ## get len \n",
    "    len_ = skills_db[real_id]['skill_len']\n",
    "    len_condition = corpus[skill_id].dot(tokens)/len_\n",
    "    \n",
    "    s_gr = np.array(list(tokens))*np.array(list(corpus[skill_id]))\n",
    "    def condition(x): return x == 1\n",
    "    s_gr_ind = [idx for idx, element in enumerate(s_gr) if condition(element)][0]\n",
    "    #print('len',len_,real_id,s_gr_ind )\n",
    "    if len_condition >=0.5 : \n",
    "        #print('debug',debug[real_id] ,corpus[skill_id] ,tokens , len_condition  )\n",
    "        if len_>2   :\n",
    "                #print('ngram', True , real_id)\n",
    "                score = len_condition\n",
    "                return (True , {'skill_id':real_id,\n",
    "                                'doc_node_id':  [i for i,val in enumerate(s_gr) if val==1],\n",
    "                                'score':round(score,2)\n",
    "                               })\n",
    "            \n",
    "        if  is_low_frequency(text[s_gr_ind],real_id)   :\n",
    "                #print('2gram', look_up_ngram[real_id],\"/\", text[s_gr_ind] ,\"/\",' '.join(text))\n",
    "                score = get_sim(skills_db[real_id]['skill_lemmed'], text[s_gr_ind] ,' '.join(text))\n",
    "                return (True , {'skill_id':real_id,\n",
    "                                'score':round(score,2),\n",
    "                                 'doc_node_id': [i for i,val in enumerate(s_gr) if val==1]\n",
    "                               })\n",
    "        return (False , '')\n",
    "\n",
    "    else :\n",
    "        return (False , '')\n",
    "    \n",
    "def get_corpus(text,matches):\n",
    "    len_ = len(text)\n",
    "    corpus = []\n",
    "    look_up = {}\n",
    "    unique_skills = list(set([match['skill_id'] for match in matches]))\n",
    "    skill_text_match_bin = [0]*len_\n",
    "    for index,skill_id in enumerate(unique_skills) : \n",
    "        \n",
    "        on_inds = [match['doc_node_id'] for match in matches if match['skill_id']==skill_id]\n",
    "        skill_text_match_bin_updated = [(i in on_inds)*1 for i,_ in enumerate(skill_text_match_bin)]\n",
    "        corpus.append(skill_text_match_bin_updated)\n",
    "        look_up[index] = skill_id\n",
    "                   \n",
    "    return np.array(corpus) , look_up\n",
    "    \n",
    "## main functions\n",
    "def process_n_gram(matches,text_obj):\n",
    "                if len(matches)==0:\n",
    "                     return matches\n",
    "                \n",
    "                # get text spans with  conflict \n",
    "                text_tokens = text_obj.lemmed().split(' ')\n",
    "                len_ = len(text_tokens)\n",
    "                ## create corpus matrix  \n",
    "                corpus,look_up = get_corpus(text_tokens,matches)\n",
    "                ## generate spans \n",
    "                co_occ = np.matmul(corpus.T, corpus) # co-occurence of tokens aij : co-occurence of token i with token j\n",
    "                clusters = get_clusters(co_occ)\n",
    "                ones = [ make_one(cluster,len_) for cluster in clusters]\n",
    "                spans = [(np.array(one),np.array([a_[0] for a_ in np.argwhere(corpus.dot(one)!=0)])) \n",
    "                         for one in ones]\n",
    "                # filter and score \n",
    "                new_spans = []\n",
    "                for span in spans :\n",
    "                    tokens,skill_ids = span \n",
    "                    new_skill_obj = []\n",
    "                    for sk_id in skill_ids : \n",
    "                        retain_ , r_sk_id = retain(text_tokens , tokens,sk_id,look_up,corpus )\n",
    "                        if   retain_ :\n",
    "                            new_skill_obj.append(r_sk_id)\n",
    "                    # get max for each span \n",
    "                    scores = [sk['score'] for sk in new_skill_obj]\n",
    "                    if scores!=[]:\n",
    "                        max_score_index = np.array(scores).argmax() \n",
    "                \n",
    "                        new_spans.append(new_skill_obj[max_score_index])                \n",
    "                \n",
    "                return new_spans\n",
    "            \n",
    "def process_unigram(matches,text_obj):\n",
    "                original_text = text_obj.transformed_text.split(' ')                \n",
    "                res = []\n",
    "                for match in matches :\n",
    "                    id_ = match['skill_id']\n",
    "                    match_id = match['doc_node_id']\n",
    "\n",
    "                    skill_str = skills_db[id_]['skill_cleaned']\n",
    "                    #print(match_id)\n",
    "                    text_str = original_text[match_id]\n",
    "                    sim = one_gram_sim(skill_str,text_str)\n",
    "                    #match['doc_node_id'] = [match['doc_node_id']] # for normalisation purpose\n",
    "\n",
    "                        \n",
    "                    res.append({'skill_id':id_,\n",
    "                                'doc_node_id':[match_id],\n",
    "                                'doc_node_value':match['doc_node_value'],\n",
    "                                'score':round(sim,2)})  \n",
    "                return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills extractor test  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  full_matcher ...\n",
      "loading  ngram_matcher ...\n",
      "loading  uni_gram_matcher ...\n"
     ]
    }
   ],
   "source": [
    "matchers = load_matchers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get matches \n",
    "input_ = 'esport management '\n",
    "text = Text(input_)\n",
    "\n",
    "skills_full,text = get_full_match_skills(text,matchers['full_matcher'])\n",
    "skills_sub_full,skills_ngram,text = get_sub_match_skills(text,matchers['ngram_matcher'])\n",
    "skills_uni = get_single_match_skills(text,matchers['uni_gram_matcher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process \n",
    "n_gram_pro = process_n_gram(skills_ngram,text)\n",
    "uni_gram_pro = process_unigram(skills_uni,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_match': [],\n",
      " 'ngram_full_match': [{'doc_node_id': [0, 1],\n",
      "                       'doc_node_value': 'esport management',\n",
      "                       'skill_id': 'ES8AAD06BE8119038221'}],\n",
      " 'ngram_scored': [],\n",
      " 'unigram_scored': []}\n"
     ]
    }
   ],
   "source": [
    "## results \n",
    "pprint.pprint({'full_match':skills_full,\n",
    "               'ngram_full_match':skills_sub_full,\n",
    "               'ngram_scored':n_gram_pro,\n",
    "               'unigram_scored':uni_gram_pro\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'skill_name': 'Advanced Disaster Management',\n",
       " 'skill_cleaned': 'advanced disaster management',\n",
       " 'skill_type': 'Hard Skill',\n",
       " 'skill_lemmed': 'advanced disaster management',\n",
       " 'skill_stemmed': 'advanc disast manag',\n",
       " 'skill_len': 3}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_db['KS1208C6D8DGHVYQTSLR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
